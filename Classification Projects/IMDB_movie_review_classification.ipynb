{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorrect-library",
   "metadata": {},
   "source": [
    "# **Movie Review Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-holiday",
   "metadata": {},
   "source": [
    "Download the dataset from this link : https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cordless-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.width',1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chubby-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "revised-prime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "professional-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only sample data \n",
    "\n",
    "df = df1.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "invalid-carry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10970</th>\n",
       "      <td>So, Todd Sheets once stated that he considers ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17368</th>\n",
       "      <td>Hint number one - read the title as \"the Time ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48891</th>\n",
       "      <td>\"Yokai Daisenso\" is a children's film by Takas...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "10970  So, Todd Sheets once stated that he considers ...  positive\n",
       "17368  Hint number one - read the title as \"the Time ...  positive\n",
       "48891  \"Yokai Daisenso\" is a children's film by Takas...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handed-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "catholic-watts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10970</td>\n",
       "      <td>So, Todd Sheets once stated that he considers ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17368</td>\n",
       "      <td>Hint number one - read the title as \"the Time ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48891</td>\n",
       "      <td>\"Yokai Daisenso\" is a children's film by Takas...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                             review sentiment\n",
       "0  10970  So, Todd Sheets once stated that he considers ...  positive\n",
       "1  17368  Hint number one - read the title as \"the Time ...  positive\n",
       "2  48891  \"Yokai Daisenso\" is a children's film by Takas...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pressing-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only review and sentiment column\n",
    "\n",
    "df = df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-brighton",
   "metadata": {},
   "source": [
    "# Data Cleaning:\n",
    "\n",
    "1) Detect which language it is and make the separate dataframe for each language\n",
    "2) Remove the puncutiation marks, numbers ..etc\n",
    "3) Make the Lower case\n",
    "4) Remove the stop words\n",
    "5) Lemittization\n",
    "6) Find a way to convert text data to numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-bacon",
   "metadata": {},
   "source": [
    "### 1) Detecting the language in each row of the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eight-findings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "detect(\"War doesn't show who's right, just who's left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "generous-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a separate column Language which displays the name of the language in each row\n",
    "\n",
    "df['Language'] = df['review'].apply(lambda x: detect(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "smoking-evans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    5000\n",
       "Name: Language, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "broken-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the 5000 samples have english sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-label",
   "metadata": {},
   "source": [
    "### 2) Removing punctuation, digits, spaces, and  making lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "brave-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "designed-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing digits\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: ''.join([i for i in x if i not in string.digits]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "artificial-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing spaces from data\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "documentary-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the Lower case\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-electron",
   "metadata": {},
   "source": [
    "### 4) Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "labeled-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words from spacy library\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: ' '.join([i for i in x.split() if i not in STOP_WORDS]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-grain",
   "metadata": {},
   "source": [
    "### 5) Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "official-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usig spacy\n",
    "import spacy\n",
    "en_core = spacy.load('en_core_web_sm')\n",
    "\n",
    "df['new_review'] = df['review'].apply(lambda x : \" \".join([y.lemma_ for y in en_core(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-census",
   "metadata": {},
   "source": [
    "### 6) Converting text data into numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "clinical-ready",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "occupied-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(max_features = 10000)\n",
    "\n",
    "tfidf_vectors = vec.fit_transform(df['new_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "quick-duncan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aamir</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abc</th>\n",
       "      <th>abduct</th>\n",
       "      <th>abe</th>\n",
       "      <th>abet</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abominable</th>\n",
       "      <th>abomination</th>\n",
       "      <th>aboriginal</th>\n",
       "      <th>abort</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abound</th>\n",
       "      <th>aboutbr</th>\n",
       "      <th>abr</th>\n",
       "      <th>abraham</th>\n",
       "      <th>abroad</th>\n",
       "      <th>abrupt</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absorb</th>\n",
       "      <th>abstract</th>\n",
       "      <th>absurd</th>\n",
       "      <th>absurdist</th>\n",
       "      <th>absurdity</th>\n",
       "      <th>absurdly</th>\n",
       "      <th>abundance</th>\n",
       "      <th>abundant</th>\n",
       "      <th>abuse</th>\n",
       "      <th>abusive</th>\n",
       "      <th>abysmal</th>\n",
       "      <th>abysmally</th>\n",
       "      <th>academic</th>\n",
       "      <th>academy</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>acceptance</th>\n",
       "      <th>access</th>\n",
       "      <th>...</th>\n",
       "      <th>yearold</th>\n",
       "      <th>yearsbr</th>\n",
       "      <th>yell</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yeti</th>\n",
       "      <th>yike</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoda</th>\n",
       "      <th>yokai</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>youbr</th>\n",
       "      <th>young</th>\n",
       "      <th>youngster</th>\n",
       "      <th>yourselfbr</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yul</th>\n",
       "      <th>yuppie</th>\n",
       "      <th>yuzna</th>\n",
       "      <th>yvonne</th>\n",
       "      <th>zadora</th>\n",
       "      <th>zane</th>\n",
       "      <th>zany</th>\n",
       "      <th>zap</th>\n",
       "      <th>zatoichi</th>\n",
       "      <th>zellweger</th>\n",
       "      <th>zephyr</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeta</th>\n",
       "      <th>zetajone</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zimmer</th>\n",
       "      <th>zit</th>\n",
       "      <th>ziyi</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zoey</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zorro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.577691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.156935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aamir  aaron   ab  abandon  abba  abbey  abbott  abc  abduct  abe  abet  abide  ability  able  aboard  abominable  abomination  aboriginal  abort  abortion  abound  aboutbr  abr  abraham  abroad  abrupt  abruptly  absence  absent  absolute  absolutely  absorb  abstract  absurd  absurdist  absurdity  absurdly  abundance  abundant  abuse  abusive  abysmal  abysmally  academic  academy  accent  accept  acceptable  acceptance  access  ...  yearold  yearsbr  yell  yellow  yep       yes  yesterday  yeti  yike   yo  yoda     yokai  york       you  youbr    young  youngster  yourselfbr  youth  youthful  youtube   yr  yrs  yuck  yul  yuppie  yuzna  yvonne  zadora  zane  zany  zap  zatoichi  zellweger  zephyr  zero  zeta  zetajone  zhang  zimmer  zit  ziyi  zodiac  zoey  zombi    zombie  zone  zoo  zoom  zorro\n",
       "0    0.0    0.0  0.0      0.0   0.0    0.0     0.0  0.0     0.0  0.0   0.0    0.0      0.0   0.0     0.0         0.0          0.0         0.0    0.0       0.0     0.0      0.0  0.0      0.0     0.0     0.0       0.0      0.0     0.0       0.0         0.0     0.0       0.0     0.0        0.0        0.0       0.0        0.0       0.0    0.0      0.0      0.0        0.0       0.0      0.0     0.0     0.0         0.0         0.0     0.0  ...      0.0      0.0   0.0     0.0  0.0  0.000000        0.0   0.0   0.0  0.0   0.0  0.000000   0.0  0.028177    0.0  0.00000        0.0         0.0    0.0       0.0      0.0  0.0  0.0   0.0  0.0     0.0    0.0     0.0     0.0   0.0   0.0  0.0       0.0        0.0     0.0   0.0   0.0       0.0    0.0     0.0  0.0   0.0     0.0   0.0    0.0  0.577691   0.0  0.0   0.0    0.0\n",
       "1    0.0    0.0  0.0      0.0   0.0    0.0     0.0  0.0     0.0  0.0   0.0    0.0      0.0   0.0     0.0         0.0          0.0         0.0    0.0       0.0     0.0      0.0  0.0      0.0     0.0     0.0       0.0      0.0     0.0       0.0         0.0     0.0       0.0     0.0        0.0        0.0       0.0        0.0       0.0    0.0      0.0      0.0        0.0       0.0      0.0     0.0     0.0         0.0         0.0     0.0  ...      0.0      0.0   0.0     0.0  0.0  0.095767        0.0   0.0   0.0  0.0   0.0  0.000000   0.0  0.072829    0.0  0.00000        0.0         0.0    0.0       0.0      0.0  0.0  0.0   0.0  0.0     0.0    0.0     0.0     0.0   0.0   0.0  0.0       0.0        0.0     0.0   0.0   0.0       0.0    0.0     0.0  0.0   0.0     0.0   0.0    0.0  0.000000   0.0  0.0   0.0    0.0\n",
       "2    0.0    0.0  0.0      0.0   0.0    0.0     0.0  0.0     0.0  0.0   0.0    0.0      0.0   0.0     0.0         0.0          0.0         0.0    0.0       0.0     0.0      0.0  0.0      0.0     0.0     0.0       0.0      0.0     0.0       0.0         0.0     0.0       0.0     0.0        0.0        0.0       0.0        0.0       0.0    0.0      0.0      0.0        0.0       0.0      0.0     0.0     0.0         0.0         0.0     0.0  ...      0.0      0.0   0.0     0.0  0.0  0.000000        0.0   0.0   0.0  0.0   0.0  0.522115   0.0  0.080175    0.0  0.12489        0.0         0.0    0.0       0.0      0.0  0.0  0.0   0.0  0.0     0.0    0.0     0.0     0.0   0.0   0.0  0.0       0.0        0.0     0.0   0.0   0.0       0.0    0.0     0.0  0.0   0.0     0.0   0.0    0.0  0.000000   0.0  0.0   0.0    0.0\n",
       "3    0.0    0.0  0.0      0.0   0.0    0.0     0.0  0.0     0.0  0.0   0.0    0.0      0.0   0.0     0.0         0.0          0.0         0.0    0.0       0.0     0.0      0.0  0.0      0.0     0.0     0.0       0.0      0.0     0.0       0.0         0.0     0.0       0.0     0.0        0.0        0.0       0.0        0.0       0.0    0.0      0.0      0.0        0.0       0.0      0.0     0.0     0.0         0.0         0.0     0.0  ...      0.0      0.0   0.0     0.0  0.0  0.000000        0.0   0.0   0.0  0.0   0.0  0.000000   0.0  0.000000    0.0  0.00000        0.0         0.0    0.0       0.0      0.0  0.0  0.0   0.0  0.0     0.0    0.0     0.0     0.0   0.0   0.0  0.0       0.0        0.0     0.0   0.0   0.0       0.0    0.0     0.0  0.0   0.0     0.0   0.0    0.0  0.000000   0.0  0.0   0.0    0.0\n",
       "4    0.0    0.0  0.0      0.0   0.0    0.0     0.0  0.0     0.0  0.0   0.0    0.0      0.0   0.0     0.0         0.0          0.0         0.0    0.0       0.0     0.0      0.0  0.0      0.0     0.0     0.0       0.0      0.0     0.0       0.0         0.0     0.0       0.0     0.0        0.0        0.0       0.0        0.0       0.0    0.0      0.0      0.0        0.0       0.0      0.0     0.0     0.0         0.0         0.0     0.0  ...      0.0      0.0   0.0     0.0  0.0  0.103181        0.0   0.0   0.0  0.0   0.0  0.000000   0.0  0.156935    0.0  0.00000        0.0         0.0    0.0       0.0      0.0  0.0  0.0   0.0  0.0     0.0    0.0     0.0     0.0   0.0   0.0  0.0       0.0        0.0     0.0   0.0   0.0       0.0    0.0     0.0  0.0   0.0     0.0   0.0    0.0  0.000000   0.0  0.0   0.0    0.0\n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(tfidf_vectors.toarray(), columns = vec.get_feature_names())\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-journey",
   "metadata": {},
   "source": [
    "## Analysing Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "independent-notion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    2522\n",
       "positive    2478\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "creative-start",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"sentiment\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-ending",
   "metadata": {},
   "source": [
    "Since the target column is balanced no need to do under or oversamapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efficient-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['sentiment'] = df['sentiment'].map({'positive':1, 'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "widespread-assist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2522\n",
       "1    2478\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "streaming-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_df, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-heavy",
   "metadata": {},
   "source": [
    "# Logestic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "regulated-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log = LogisticRegression(max_iter = 200)\n",
    "log_model = log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "transparent-watts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-display",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aquatic-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we take k value 1 then we check the model then in the below we can understand how to select k value\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) # n_neighbors is the k value\n",
    "knn_model = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "radical-graduation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "patent-burns",
   "metadata": {},
   "source": [
    "Takes more time so I am making it raw (but if we execute we will get the output)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "rational-following",
   "metadata": {},
   "source": [
    "# predicting the right k value\n",
    "error_rate = []\n",
    "\n",
    "for i in range(1, 40):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn_model = knn.fit(X_train, y_train)\n",
    "    pred_i = knn_model.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "therapeutic-satin",
   "metadata": {},
   "source": [
    "plt.figure(figsize =(10, 6))\n",
    "plt.plot(range(1, 40), error_rate, color ='blue',\n",
    "                linestyle ='dashed', marker ='o',\n",
    "         markerfacecolor ='red', markersize = 10)\n",
    "  \n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K value')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-steam",
   "metadata": {},
   "source": [
    "As we can see in the graph at k value  the error rate is less and it is stable to some extinct so we can select k value as "
   ]
  },
  {
   "cell_type": "raw",
   "id": "institutional-shaft",
   "metadata": {},
   "source": [
    "# making K value as \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) # n_neighbors is the k value\n",
    "knn_model = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "raising-store",
   "metadata": {},
   "source": [
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-strand",
   "metadata": {},
   "source": [
    "# Naive Bayes - Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "desirable-driver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "following-reply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.642\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-february",
   "metadata": {},
   "source": [
    "# Naive Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "above-sender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_model1 = MultinomialNB()\n",
    "nb_model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "played-walnut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842\n"
     ]
    }
   ],
   "source": [
    "y_pred_m = nb_model1.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred_m, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-theme",
   "metadata": {},
   "source": [
    "# SVM (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "hired-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear')\n",
    "svm_model = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "interested-fighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-category",
   "metadata": {},
   "source": [
    "# SVM (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "imposed-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf')\n",
    "svm_model_rbf = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "sound-probe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model_rbf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-fancy",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fallen-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_model = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adjustable-roulette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.682\n"
     ]
    }
   ],
   "source": [
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-living",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "stretch-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "least-terry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-drinking",
   "metadata": {},
   "source": [
    "# Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "secondary-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ab = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "ab_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "modified-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813\n"
     ]
    }
   ],
   "source": [
    "y_pred = ab_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-consolidation",
   "metadata": {},
   "source": [
    "# XG-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "understood-graphic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:50:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "xg = xgboost.XGBClassifier()\n",
    "xgboost_model = xg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "therapeutic-writing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813\n"
     ]
    }
   ],
   "source": [
    "y_pred = ab_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-armenia",
   "metadata": {},
   "source": [
    "# XG-Boost By Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "careful-closure",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "august-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = xgboost.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "opponent-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    " \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "significant-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(classifier, param_distributions=params, n_iter=5, scoring='roc_auc', n_jobs=1, cv=3, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "arbitrary-blast",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] min_child_weight=3, max_depth=3, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:52:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=3, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3, score=0.889, total=   8.7s\n",
      "[CV] min_child_weight=3, max_depth=3, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.7s remaining:    0.0s\n",
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:52:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=3, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3, score=0.888, total=  12.4s\n",
      "[CV] min_child_weight=3, max_depth=3, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   21.2s remaining:    0.0s\n",
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:52:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=3, learning_rate=0.3, gamma=0.2, colsample_bytree=0.3, score=0.892, total=  11.8s\n",
      "[CV] min_child_weight=1, max_depth=10, learning_rate=0.05, gamma=0.1, colsample_bytree=0.5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:53:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=1, max_depth=10, learning_rate=0.05, gamma=0.1, colsample_bytree=0.5, score=0.877, total=  25.9s\n",
      "[CV] min_child_weight=1, max_depth=10, learning_rate=0.05, gamma=0.1, colsample_bytree=0.5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:53:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=1, max_depth=10, learning_rate=0.05, gamma=0.1, colsample_bytree=0.5, score=0.874, total=  26.4s\n",
      "[CV] min_child_weight=1, max_depth=10, learning_rate=0.05, gamma=0.1, colsample_bytree=0.5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:53:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=1, max_depth=10, learning_rate=0.05, gamma=0.1, colsample_bytree=0.5, score=0.876, total=  26.5s\n",
      "[CV] min_child_weight=7, max_depth=5, learning_rate=0.25, gamma=0.1, colsample_bytree=0.5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:54:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=7, max_depth=5, learning_rate=0.25, gamma=0.1, colsample_bytree=0.5, score=0.890, total=  16.6s\n",
      "[CV] min_child_weight=7, max_depth=5, learning_rate=0.25, gamma=0.1, colsample_bytree=0.5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:54:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=7, max_depth=5, learning_rate=0.25, gamma=0.1, colsample_bytree=0.5, score=0.887, total=  16.6s\n",
      "[CV] min_child_weight=7, max_depth=5, learning_rate=0.25, gamma=0.1, colsample_bytree=0.5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:54:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=7, max_depth=5, learning_rate=0.25, gamma=0.1, colsample_bytree=0.5, score=0.885, total=  16.7s\n",
      "[CV] min_child_weight=3, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:55:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.3, score=0.896, total=  14.2s\n",
      "[CV] min_child_weight=3, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:55:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.3, score=0.891, total=  14.3s\n",
      "[CV] min_child_weight=3, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:55:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=6, learning_rate=0.2, gamma=0.2, colsample_bytree=0.3, score=0.887, total=  14.3s\n",
      "[CV] min_child_weight=3, max_depth=12, learning_rate=0.15, gamma=0.4, colsample_bytree=0.5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:55:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=12, learning_rate=0.15, gamma=0.4, colsample_bytree=0.5, score=0.898, total=  30.5s\n",
      "[CV] min_child_weight=3, max_depth=12, learning_rate=0.15, gamma=0.4, colsample_bytree=0.5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:56:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=12, learning_rate=0.15, gamma=0.4, colsample_bytree=0.5, score=0.891, total=  31.0s\n",
      "[CV] min_child_weight=3, max_depth=12, learning_rate=0.15, gamma=0.4, colsample_bytree=0.5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:56:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV]  min_child_weight=3, max_depth=12, learning_rate=0.15, gamma=0.4, colsample_bytree=0.5, score=0.895, total=  31.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  5.0min finished\n",
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:57:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100,...\n",
       "                                           reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=5, n_jobs=1,\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5,\n",
       "                                                             0.7],\n",
       "                                        'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
       "                                                          0.25, 0.3],\n",
       "                                        'max_depth': [3, 4, 5, 6, 8, 10, 12,\n",
       "                                                      15],\n",
       "                                        'min_child_weight': [1, 3, 5, 7]},\n",
       "                   scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "biblical-special",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0.4, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.15, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=3, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "experimental-anime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 3,\n",
       " 'max_depth': 12,\n",
       " 'learning_rate': 0.15,\n",
       " 'gamma': 0.4,\n",
       " 'colsample_bytree': 0.5}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "inside-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier_model = xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.5, gamma=0.4, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.15, max_delta_step=0, max_depth=12,\n",
    "              min_child_weight=3, missing=np.nan, monotone_constraints='()',\n",
    "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "academic-charger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:11:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0.4, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.15, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=3, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "powerful-blame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_classifier_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-indication",
   "metadata": {},
   "source": [
    "## If you want to run the model using cross validation then below is the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "motivated-helen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manoj\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:12:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:13:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:14:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(best_classifier_model, new_df, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "satisfactory-territory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81583683, 0.81583683, 0.80972389])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "material-smith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.813799184940923"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-palace",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "\n",
    "When we compare the scores of all models, for SVM (RBF) we got high accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-doctor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
